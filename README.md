# Azure RAG Chatbot (Retrieval-Augmented Generation)

An end-to-end Retrieval-Augmented Generation (RAG) chatbot built using **Azure OpenAI**, **Azure AI Search**, and **Azure Document Intelligence**.

This project demonstrates how to ground Large Language Model (LLM) responses in enterprise data to prevent hallucinations.

---

## ðŸ”¹ Architecture

User Query  
â†’ Embeddings (Azure OpenAI)  
â†’ Vector Search (Azure AI Search)  
â†’ Relevant Context  
â†’ GPT-4o (Azure OpenAI)  
â†’ Grounded Answer  

---

## ðŸ”¹ Technologies Used

- Azure OpenAI (GPT-4o, text-embedding-3-small)
- Azure AI Search (Vector Index, HNSW)
- Azure AI Foundry (Model deployments)
- Azure Document Intelligence (OCR â€“ optional)
- Python

---

## ðŸ”¹ Features

- Vector-based semantic search
- Hallucination-free answers
- Manual text ingestion
- Modular & extensible design
- AI-102 (Azure AI Engineer Associate) aligned

---

## ðŸ”¹ Project Structure

rag-chatbot/
â”œâ”€â”€ create_index.py
â”œâ”€â”€ ingest_docs.py
â”œâ”€â”€ chat.py
â”œâ”€â”€ ocr_pdf.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md


---

## ðŸ”¹ How to Run

1. Clone the repository
2. Create `.env` from `.env.example`
3. Install dependencies:
```bash
pip install -r requirements.txt

4.Create vector index:
python create_index.py

5.Ingest documents:
python ingest_docs.py

6. Start Chat
python chat.py

ðŸ”¹ Sample Questions

How many casual leaves are allowed?
What are office working hours?
Is work from home allowed?

ðŸ”¹ Interview Highlights

Uses VectorizedQuery (latest Azure SDK)
Separate embedding and chat deployments
Azure AI Foundry-based deployment workflow
Cost-aware architecture

ðŸ“Œ Author

Anbalagan Mannan
Azure AI Engineer | .NET Developer